<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="htmlgraph-version" content="1.0">
    <title>Multi-AI Orchestration via Headless CLI Modes</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <article id="spk-a5efe38b"
             data-type="spike"
             data-status="todo"
             data-priority="medium"
             data-created="2026-01-02T19:17:09.320424"
             data-updated="2026-01-02T19:17:09.320428" data-spike-type="architectural" data-timebox-hours="3">

        <header>
            <h1>Multi-AI Orchestration via Headless CLI Modes</h1>
            <div class="metadata">
                <span class="badge status-todo">Todo</span>
                <span class="badge priority-medium">Medium Priority</span>
            </div>
        </header>

    
        <section data-spike-metadata>
            <h3>Spike Metadata</h3>
            <dl>
                <dt>Type</dt>
                <dd>Architectural</dd>
                <dt>Timebox</dt>
                <dd>3 hours</dd>
            </dl>
        </section>
        <section data-findings>
            <h3>Findings</h3>
            <div class="findings-content">
                # Multi-AI Orchestration via Headless CLI Modes

## Executive Summary

Design a system where Claude Code acts as orchestrator, delegating work to ANY AI with a headless CLI mode (Gemini, Copilot, Cursor, OpenCode, Ollama) using Bash tool. This leverages generous free tiers instead of requiring API keys.

## Researched AI CLI Capabilities

### 1. Gemini CLI (READY)

**Headless Mode:** ✅ Fully supported
**Command:**
```bash
gemini -p "Your prompt here" --output-format json
```

**Features:**
- `-p` or `--prompt` for non-interactive mode
- `--output-format json` for structured output
- `--output-format stream-json` for real-time events
- No context between invocations (stateless)

**Limitations:**
- Custom commands not working in headless mode
- No persistent session state

**Sources:**
- [Gemini CLI Headless Docs](https://geminicli.com/docs/cli/headless/)
- [GitHub Repo](https://github.com/google-gemini/gemini-cli)

### 2. GitHub Copilot CLI (READY)

**Headless Mode:** ✅ Supported with flags
**Command:**
```bash
gh copilot --allow-tool 'shell' --allow-tool 'write' "Your task here"
```

**Features:**
- `--allow-tool 'shell'` - auto-approve shell commands
- `--allow-tool 'write'` - auto-approve file edits
- `--deny-tool 'shell(rm)'` - block specific commands
- Powered by Claude Sonnet 4.5

**Security:**
- Run in VM/container for safety
- Use deny-tool to block dangerous operations

**Sources:**
- [GitHub Copilot CLI Docs](https://docs.github.com/en/copilot/how-tos/use-copilot-agents/use-copilot-cli)
- [Copilot CLI on GitHub](https://github.com/features/copilot/cli)

### 3. Cursor CLI (AVAILABLE, ROUGH EDGES)

**Headless Mode:** ⚠️ Supported but unstable
**Command:**
```bash
cursor-agent chat "find one bug and fix it"
```

**Issues:**
- May hang indefinitely
- --print mode can still block
- OpenCode recommended as more stable alternative

**Sources:**
- [Cursor CLI Docs](https://cursor.com/docs/cli/headless)
- [Forum Discussion](https://forum.cursor.com/t/cursor-cli-headless-mode-does-not-release-the-terminal/133624)

### 4. OpenCode CLI (READY, RECOMMENDED)

**Headless Mode:** ✅ Fully supported
**Command:**
```bash
# Headless server with HTTP API
opencode --headless

# Headless with web interface
opencode --headless --web
```

**Features:**
- HTTP server for API access
- More stable than Cursor for headless use
- Web interface option

**Sources:**
- [OpenCode CLI Docs](https://opencode.ai/docs/cli/)
- [Cursor Docs Recommendation](https://docs.cursor.com/en/cli/headless)

### 5. Ollama (READY, LOCAL)

**Headless Mode:** ✅ Fully supported
**Commands:**
```bash
# Batch mode
ollama run llama3.2 "Your prompt" > output.txt

# Server mode
ollama serve  # HTTP API on localhost:11434
```

**Features:**
- Batch mode for one-shot prompts
- Server mode for HTTP API
- Perfect for local, offline work
- Free, no API keys

**Sources:**
- [Ollama CLI Tutorial](https://www.hostinger.com/tutorials/ollama-cli-tutorial)
- [Ollama Commands Guide](https://geshan.com.np/blog/2025/02/ollama-commands/)

## Architecture Design

### Multi-AI Orchestrator Pattern

```
Claude Code (Orchestrator)
  - Detects available AI CLIs
  - Delegates work via Bash
  - Aggregates results
  |
  ├─────► Gemini CLI (headless, free tier)
  ├─────► Copilot CLI (headless, GitHub auth)
  ├─────► OpenCode CLI (headless server)
  ├─────► Ollama (local, offline)
  └─────► Claude subagents (Task tool)
```

### SDK API Design

```python
from htmlgraph import SDK

sdk = SDK(agent='orchestrator')

# Detect available AIs
available = sdk.detect_ai_clis()
# Returns: ['gemini', 'copilot', 'opencode', 'ollama', 'claude']

# Delegate to any AI
result = sdk.delegate_to_ai(
    ai='gemini',
    prompt='Analyze these dependencies',
    context_files=['pyproject.toml', 'requirements.txt'],
    output_format='spike',  # or 'json', 'file', 'stdout'
    worktree=True  # Optional: create git worktree
)

# Parallel delegation to multiple AIs
results = sdk.delegate_parallel([
    ('gemini', 'Analyze dependencies'),
    ('copilot', 'Generate tests'),
    ('ollama', 'Write documentation')
])

# Cost-optimized routing
result = sdk.delegate_smart(
    prompt='Complex analysis task',
    prefer_free=True,  # Try free AIs first
    fallback='claude'  # Fallback to Claude if free fails
)
```

## Implementation Roadmap

### Phase 1: AI Detection & Registry (1 day)

- Create AIRegistry class
- Implement detect_installed() for each AI CLI
- Add capability detection (headless, output formats, tools)
- Create configuration file for AI CLI paths

### Phase 2: Headless Spawning (2 days)

- Create HeadlessSpawner class
- Implement spawn_gemini() with JSON output
- Implement spawn_copilot() with auto-approvals
- Implement spawn_opencode() with HTTP API
- Implement spawn_ollama() with batch mode
- Add error handling and timeout logic

### Phase 3: Result Collection (1 day)

- Create ResultCollector class
- Parse JSON, Markdown, plain text outputs
- Auto-save to spikes/features in HtmlGraph
- Handle failures gracefully

### Phase 4: SDK Integration (2 days)

- Add delegate_to_ai() to SDK
- Add delegate_parallel() for multi-AI
- Add delegate_smart() for cost optimization
- Add conflict detection for parallel work

### Phase 5: Git Worktree Integration (2 days)

- Auto-create worktrees for delegated work
- Isolate each AI in its own branch
- Merge results back to main
- Clean up worktrees after completion

### Phase 6: HtmlGraph Extensions (1 day)

- Verify Codex skill works with SDK
- Verify Gemini extension works with SDK
- Verify OpenCode extension works with SDK
- Document agent-specific setup

## Cost Optimization Strategy

**Use free tiers by routing smartly:**

| Task Type | Best AI | Why |
|-----------|---------|-----|
| Analysis, Research | Gemini | Free 2.0 Flash, generous limits |
| Code Generation | Copilot | GitHub free tier, Sonnet 4.5 |
| Local/Sensitive | Ollama | Free, offline, private |
| Complex Reasoning | Claude | Best quality, use sparingly |

## Security Considerations

1. **Copilot Auto-Approvals**: Run in VM/container when using `--allow-tool`
2. **File Access**: Limit which files AIs can read/write
3. **Shell Commands**: Use `--deny-tool` to block dangerous operations
4. **API Keys**: Never pass secrets to AI prompts
5. **Git Worktrees**: Isolate each AI in separate branch

            </div>
        </section>
        <section data-decision>
            <h3>Decision</h3>
            <p>
**Priority Implementation Order:**

1. **Gemini CLI Integration** (FIRST) - Most stable, best docs
2. **Ollama Integration** (SECOND) - Local, offline, free
3. **Copilot CLI Integration** (THIRD) - Good for code tasks
4. **OpenCode Integration** (FOURTH) - Stable alternative to Cursor

**Skip for now:**
- Cursor CLI (too unstable)

**Architecture:**
- Build around AIRegistry + HeadlessSpawner pattern
- Use file-based handoff for complex tasks
- Integrate with git worktrees for isolation

**Next Actions:**
1. Create AIRegistry class
2. Test Gemini CLI headless mode
3. Implement spawn_gemini() method
</p>
        </section>
    </article>
</body>
</html>
