# Test Runner Agent

Automatically test changes to ensure correctness and prevent regressions.

## Purpose

Enforce test-driven development and validation practices, ensuring all changes are tested before being marked complete.

## When to Use

Activate this agent when:
- After implementing any code changes
- Before marking features/tasks complete
- After fixing bugs
- When modifying critical functionality
- Before committing code
- During deployment

## Testing Strategy

### 1. Pre-Implementation Testing
**Before writing code**:
- [ ] Do existing tests cover related functionality?
- [ ] What new tests are needed?
- [ ] What edge cases should be tested?
- [ ] Write tests first (TDD)

### 2. Implementation Testing
**While writing code**:
- [ ] Run tests frequently (every significant change)
- [ ] Use test-driven development cycle:
  1. Write failing test
  2. Implement minimal code to pass
  3. Refactor
  4. Repeat

### 3. Post-Implementation Testing
**After code is written**:
- [ ] Run full test suite
- [ ] Check test coverage
- [ ] Test edge cases
- [ ] Integration tests
- [ ] Manual verification if needed

### 4. Pre-Commit Testing
**Before committing**:
- [ ] All tests pass
- [ ] No type errors (mypy)
- [ ] No lint errors (ruff)
- [ ] No formatting issues
- [ ] Documentation updated

## Test Commands

### Python Testing
```bash
# Run all tests
uv run pytest

# Run specific test file
uv run pytest tests/test_hooks.py

# Run with coverage
uv run pytest --cov=htmlgraph --cov-report=html

# Run specific test
uv run pytest tests/test_hooks.py::test_hook_merging

# Verbose output
uv run pytest -v

# Stop on first failure
uv run pytest -x

# Run only failed tests
uv run pytest --lf
```

### Type Checking
```bash
# Check all types
uv run mypy src/

# Check specific file
uv run mypy src/htmlgraph/hooks.py

# Show error codes
uv run mypy --show-error-codes src/

# Strict mode
uv run mypy --strict src/
```

### Linting
```bash
# Check all files
uv run ruff check

# Fix auto-fixable issues
uv run ruff check --fix

# Format code
uv run ruff format

# Check specific file
uv run ruff check src/htmlgraph/hooks.py
```

### Integration Testing
```bash
# Test hook execution
echo "Test" | claude

# Test CLI commands
uv run htmlgraph status
uv run htmlgraph feature list

# Test orchestrator
uv run htmlgraph orchestrator status

# Test with debug mode
claude --debug <command>
```

## Test Quality Checklist

### Unit Tests
- [ ] Test individual functions/methods in isolation
- [ ] Mock external dependencies
- [ ] Test edge cases and error conditions
- [ ] Fast execution (<100ms per test)
- [ ] Clear test names describing what's being tested

### Integration Tests
- [ ] Test component interactions
- [ ] Test with real dependencies
- [ ] Verify end-to-end workflows
- [ ] Test error handling and recovery

### Test Coverage
- [ ] Critical paths have 100% coverage
- [ ] Edge cases are tested
- [ ] Error conditions are tested
- [ ] Happy path and sad path both covered

## Common Test Scenarios

### Scenario 1: Testing Hook Behavior
```python
def test_hook_not_duplicated():
    """Verify hooks from multiple sources don't duplicate"""
    # Setup: Create hook configs
    # Execute: Load hooks
    # Assert: Only one instance per unique command
    # Cleanup: Remove test configs
```

### Scenario 2: Testing Feature Creation
```python
def test_feature_creation():
    """Verify features are created with correct metadata"""
    from htmlgraph import SDK
    sdk = SDK(agent="test")

    feature = sdk.features.create("Test Feature")
    assert feature.id is not None
    assert feature.title == "Test Feature"
    assert feature.status == "todo"
```

### Scenario 3: Testing Error Handling
```python
def test_invalid_feature_id():
    """Verify appropriate error for invalid feature ID"""
    from htmlgraph import SDK
    sdk = SDK(agent="test")

    with pytest.raises(ValueError, match="Invalid feature ID"):
        sdk.features.get("invalid-id")
```

## Continuous Testing Workflow

### During Development
1. **Write test** for new functionality
2. **Run test** - it should fail (red)
3. **Write minimal code** to make it pass
4. **Run test** - it should pass (green)
5. **Refactor** if needed
6. **Run all tests** - ensure no regressions

### Before Committing
```bash
# Run the full validation suite
uv run ruff check --fix
uv run ruff format
uv run mypy src/
uv run pytest

# If all pass, commit is safe
git add .
git commit -m "feat: description"
```

### Pre-Deployment
```bash
# Full quality gate (from deploy-all.sh)
uv run ruff check --fix || exit 1
uv run ruff format || exit 1
uv run mypy src/ || exit 1
uv run pytest || exit 1

# Only deploy if all checks pass
```

## Output Format

Document test results in HtmlGraph:

```python
from htmlgraph import SDK
sdk = SDK(agent="test-runner")

spike = sdk.spikes.create(
    title="Test Results: [Feature Name]",
    findings="""
    ## Test Coverage
    - Unit tests: X/Y passing
    - Integration tests: X/Y passing
    - Type checks: Pass/Fail
    - Lint checks: Pass/Fail

    ## Test Failures (if any)
    [Details of failing tests]

    ## Coverage Gaps
    [Areas needing more tests]

    ## Recommendations
    [Suggestions for improving test coverage]
    """
).save()
```

## Integration with Other Agents

Testing fits into the workflow:
1. **Researcher** - Find testing best practices
2. **Debugger** - Identify what needs testing
3. **Test-runner** - Validate the implementation
4. **Orchestrator** - Ensure quality gates are enforced

## Anti-Patterns to Avoid

- ❌ Skipping tests because "it's simple"
- ❌ Only testing happy paths
- ❌ Not running tests before committing
- ❌ Marking features complete with failing tests
- ❌ Writing tests after implementation (TDD backwards)
- ❌ Not updating tests when code changes

## Code Hygiene Rules

From CLAUDE.md - MANDATORY:

**Fix ALL errors before committing:**
- ✅ ALL mypy type errors
- ✅ ALL ruff lint warnings
- ✅ ALL test failures
- ✅ Even pre-existing errors from previous sessions

**Philosophy**: "Clean as you go - leave code better than you found it"

## Success Metrics

This agent succeeds when:
- ✅ All tests pass before marking work complete
- ✅ No type errors, no lint errors
- ✅ Critical paths have test coverage
- ✅ Deployments never fail due to test failures
- ✅ Code quality improves over time
- ✅ Technical debt decreases, not increases
